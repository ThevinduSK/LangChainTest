{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The temperature parameter in the OpenAI class controls the \\n    randomness of the generated text. A lower temperature (closer to 0) \\n    makes the output more deterministic and focused, while \\n    a higher temperature (closer to 1) makes the output more random and creative.\\n    '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\"\"\"The temperature parameter in the OpenAI class controls the \n",
    "    randomness of the generated text. A lower temperature (closer to 0) \n",
    "    makes the output more deterministic and focused, while \n",
    "    a higher temperature (closer to 1) makes the output more random and creative.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_36976\\671280666.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Italy - The birthplace of pasta, Italy offers a wide variety of delicious pasta dishes, from classic spaghetti carbonara to regional specialties like trofie al pesto and orecchiette with sausage and broccoli rabe.\n",
      "\n",
      "2. New York City, USA - With a large Italian-American population, New York City is known for its delicious and diverse pasta options. From upscale pasta restaurants to casual delis and pizzerias, there are endless options for pasta lovers.\n",
      "\n",
      "3. Tuscany, Italy - Known for its rustic and hearty cuisine, Tuscany is a must-visit for anyone who loves pasta. Try traditional dishes like pappardelle alla lepre (pasta with wild hare ragu) and ribollita (vegetable and bread soup).\n",
      "\n",
      "4. Emilia-Romagna, Italy - This region is known as the \"breadbasket of Italy\" and is home to some of the most iconic pasta dishes, such as tagliatelle al ragu (bolognese sauce) and tortellini in brodo (pasta in broth).\n",
      "\n",
      "5. Tokyo, Japan - Surprised to see Tokyo on this list? While not traditionally known for its pasta, Tokyo has a thriving Italian food scene with numerous upscale\n"
     ]
    }
   ],
   "source": [
    "text = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates: Manage Prompts for LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"food\"],\n",
    "    template = \"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat dessert?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food=\"dessert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France - known for its delectable pastries, macarons, and crepes\n",
      "2. New York City, USA - home to diverse and creative dessert options, such as cronuts and cheesecake\n",
      "3. Tokyo, Japan - famous for its unique and intricate dessert creations, such as matcha green tea desserts\n",
      "4. Istanbul, Turkey - a haven for anyone with a sweet tooth, with its variety of baklava and Turkish delight\n",
      "5. Rome, Italy - famous for gelato and its traditional Italian desserts like tiramisu and cannoli.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food=\"dessert\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains : Combine LLMs and prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "propmt = PromptTemplate(\n",
    "    input_variables = [\"food\"],\n",
    "    template = \"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_36976\\3613428796.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=propmt)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=propmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_36976\\79267893.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain.run(\"fruit\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Hawaii: Hawaii is known for its vast array of tropical fruits, including pineapples, mangoes, and papayas.\n",
      "\n",
      "2. Thailand: Thailand is home to a variety of exotic fruits such as durian, mangoes, and lychees, which can be found at local markets and street vendors.\n",
      "\n",
      "3. Greece: The Mediterranean climate in Greece makes it an ideal destination for fruit lovers, with an abundance of fresh and flavorful fruits such as figs, grapes, and olives.\n",
      "\n",
      "4. Costa Rica: Costa Rica is a fruit lover's paradise, with an incredible variety of tropical fruits such as bananas, passion fruit, and guavas.\n",
      "\n",
      "5. Italy: Italy not only offers delicious pasta and pizza, but also has a rich selection of seasonal and locally grown fruits such as oranges, lemons, and figs.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"fruit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents : Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"83c1b43a39ab8194755503221ecacbeed1621d1d60dcb8bcce5bf3e6c65f01e7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "# 1. The tools\n",
    "# 2. The LLM\n",
    "# 3. The type of agent we want to use\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the search engine to find information about the current leader of Japan.\n",
      "Action: Search\n",
      "Action Input: \"current leader of Japan\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe current prime minister is Shigeru Ishiba, who assumed office on 1 October 2024. There are currently eleven living former prime ministers. The most recent former prime minister to die, Shinzo Abe, was assassinated on 8 July 2022.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the search engine again to find information about Shigeru Ishiba's age.\n",
      "Action: Search\n",
      "Action Input: \"Shigeru Ishiba age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Shigeru Ishiba › Age ShareSend feedbackClose menu'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the calculator to calculate Shigeru Ishiba's age.\n",
      "Action: Calculator\n",
      "Action Input: \"2024 - 1957\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 67\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Shigeru Ishiba is 67 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shigeru Ishiba is 67 years old.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the current leader of Japan? What is his age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory: Add state to chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0) # temp means randomness. closer to 0 deterministic. closer to 1 creative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_36976\\526575945.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, verbose=True)\n",
      "c:\\Conda_Environment\\envs\\myenv1\\Lib\\site-packages\\pydantic\\main.py:214: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The first thing you said to me was \"Hi there!\" It was a friendly greeting and I responded in kind. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:  The first thing you said to me was \"Hi there!\" It was a friendly greeting and I responded in kind. Is there anything else you would like to know?\n",
      "Human: What is an alternative phrase for the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative phrase for \"Hi there!\" could be \"Hello!\" or \"Greetings!\" Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is an alternative phrase for the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
