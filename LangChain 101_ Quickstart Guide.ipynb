{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The temperature parameter in the OpenAI class controls the \\n    randomness of the generated text. A lower temperature (closer to 0) \\n    makes the output more deterministic and focused, while \\n    a higher temperature (closer to 1) makes the output more random and creative.\\n    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\"\"\"The temperature parameter in the OpenAI class controls the \n",
    "    randomness of the generated text. A lower temperature (closer to 0) \n",
    "    makes the output more deterministic and focused, while \n",
    "    a higher temperature (closer to 1) makes the output more random and creative.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_36976\\671280666.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Italy - The birthplace of pasta, Italy offers a wide variety of delicious pasta dishes, from classic spaghetti carbonara to regional specialties like trofie al pesto and orecchiette with sausage and broccoli rabe.\n",
      "\n",
      "2. New York City, USA - With a large Italian-American population, New York City is known for its delicious and diverse pasta options. From upscale pasta restaurants to casual delis and pizzerias, there are endless options for pasta lovers.\n",
      "\n",
      "3. Tuscany, Italy - Known for its rustic and hearty cuisine, Tuscany is a must-visit for anyone who loves pasta. Try traditional dishes like pappardelle alla lepre (pasta with wild hare ragu) and ribollita (vegetable and bread soup).\n",
      "\n",
      "4. Emilia-Romagna, Italy - This region is known as the \"breadbasket of Italy\" and is home to some of the most iconic pasta dishes, such as tagliatelle al ragu (bolognese sauce) and tortellini in brodo (pasta in broth).\n",
      "\n",
      "5. Tokyo, Japan - Surprised to see Tokyo on this list? While not traditionally known for its pasta, Tokyo has a thriving Italian food scene with numerous upscale\n"
     ]
    }
   ],
   "source": [
    "text = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates: Manage Prompts for LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"food\"],\n",
    "    template = \"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat dessert?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food=\"dessert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France - known for its delectable pastries, macarons, and crepes\n",
      "2. New York City, USA - home to diverse and creative dessert options, such as cronuts and cheesecake\n",
      "3. Tokyo, Japan - famous for its unique and intricate dessert creations, such as matcha green tea desserts\n",
      "4. Istanbul, Turkey - a haven for anyone with a sweet tooth, with its variety of baklava and Turkish delight\n",
      "5. Rome, Italy - famous for gelato and its traditional Italian desserts like tiramisu and cannoli.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food=\"dessert\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains : Combine LLMs and prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "propmt = PromptTemplate(\n",
    "    input_variables = [\"food\"],\n",
    "    template = \"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_37956\\3613428796.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=propmt)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=propmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_36976\\79267893.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain.run(\"fruit\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Hawaii: Hawaii is known for its vast array of tropical fruits, including pineapples, mangoes, and papayas.\n",
      "\n",
      "2. Thailand: Thailand is home to a variety of exotic fruits such as durian, mangoes, and lychees, which can be found at local markets and street vendors.\n",
      "\n",
      "3. Greece: The Mediterranean climate in Greece makes it an ideal destination for fruit lovers, with an abundance of fresh and flavorful fruits such as figs, grapes, and olives.\n",
      "\n",
      "4. Costa Rica: Costa Rica is a fruit lover's paradise, with an incredible variety of tropical fruits such as bananas, passion fruit, and guavas.\n",
      "\n",
      "5. Italy: Italy not only offers delicious pasta and pizza, but also has a rich selection of seasonal and locally grown fruits such as oranges, lemons, and figs.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"fruit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents : Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"83c1b43a39ab8194755503221ecacbeed1621d1d60dcb8bcce5bf3e6c65f01e7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_37956\\1179223893.py:6: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. See LangGraph documentation for more details: https://langchain-ai.github.io/langgraph/. Refer here for its pre-built ReAct agent: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "  agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose= True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "# 1. The tools\n",
    "# 2. The LLM\n",
    "# 3. The type of agent we want to use\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the search engine to find information about the current leader of Japan.\n",
      "Action: Search\n",
      "Action Input: \"current leader of Japan\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe current prime minister is Shigeru Ishiba, who assumed office on 1 October 2024. There are currently eleven living former prime ministers. The most recent former prime minister to die, Shinzo Abe, was assassinated on 8 July 2022.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the search engine again to find information about Shigeru Ishiba's age.\n",
      "Action: Search\n",
      "Action Input: \"Shigeru Ishiba age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Shigeru Ishiba › Age ShareSend feedbackClose menu'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use the calculator to calculate Shigeru Ishiba's age.\n",
      "Action: Calculator\n",
      "Action Input: \"2024 - 1957\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 67\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Shigeru Ishiba is 67 years old.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shigeru Ishiba is 67 years old.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the current leader of Japan? What is his age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory: Add state to chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0) # temp means randomness. closer to 0 deterministic. closer to 1 creative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_37956\\526575945.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, verbose=True)\n",
      "c:\\Conda_Environment\\envs\\myenv1\\Lib\\site-packages\\pydantic\\main.py:214: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The first thing you said to me was \"Hi there!\" It was a friendly greeting and I responded in kind. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:  The first thing you said to me was \"Hi there!\" It was a friendly greeting and I responded in kind. Is there anything else you would like to know?\n",
      "Human: What is an alternative phrase for the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' An alternative phrase for \"Hi there!\" could be \"Hello!\" or \"Greetings!\" Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is an alternative phrase for the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.SERP API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\"serpapi\"]\n",
    "tools = load_tools(tool_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thevindu Kalubowila\\AppData\\Local\\Temp\\ipykernel_37956\\2930992816.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"What is Langchain?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Langchain is probably a term or name that I am not familiar with.\n",
      "Action: Search\n",
      "Action Input: \"Langchain\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['LangChain type: Software.', 'LangChain entity_type: kp3_verticals.', 'LangChain kgmid: /g/11kjpl7_60.', 'LangChain initial_release_date: October 2022.', 'LangChain developer_s: Harrison Chase.', 'LangChain license: MIT License.', 'LangChain repository: github.com/langchain-ai/langchain.', 'LangChain stable_release: 0.1.16 / 11 April 2024; 7 months ago.', 'LangChain written_in: Python and JavaScript.', 'From startups to global enterprises, ambitious builders choose LangChain products. Build. LangChain is a composable framework to build with LLMs. LangGraph is ...', 'LangChain is a framework for developing applications powered by large language models (LLMs). For these applications, LangChain simplifies the entire ...', 'LangChain provides AI developers with tools to connect language models with external data sources. It is open-source and supported by an active community.', \"We've rebranded our service for deploying and scaling LangGraph apps as LangGraph Platform. Learn about the multiple deployment options and what LangGraph ...\", 'LangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models.', \"Build context-aware, reasoning applications with LangChain's flexible abstractions and AI-first toolkit.\"]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is a lot of information about Langchain. It seems to be a software framework for building applications with large language models.\n",
      "Action: Search\n",
      "Action Input: \"Langchain applications\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"LangChain is a software framework that helps facilitate the integration of large language models into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", 'LangChain type: Software.', 'LangChain entity_type: kp3_verticals.', 'LangChain kgmid: /g/11kjpl7_60.', 'LangChain initial_release_date: October 2022.', 'LangChain developer_s: Harrison Chase.', 'LangChain license: MIT License.', 'LangChain repository: github.com/langchain-ai/langchain.', 'LangChain stable_release: 0.1.16 / 11 April 2024; 7 months ago.', 'LangChain written_in: Python and JavaScript.', \"Build your app with LangChain. Build context-aware, reasoning applications with LangChain's flexible framework that leverages your company's data and APIs.\", 'LangChain is a framework for developing applications powered by large language models (LLMs). LangChain simplifies every stage of the LLM application lifecycle:.', 'LangChain is a framework for developing applications powered by large language models (LLMs). For these applications, LangChain simplifies the entire ...', 'An automated tool that monitors government websites to notify users of relevant benefits. It provides real-time, language-agnostic updates, simplifying access ...', 'LangChain offers a robust framework for seamlessly leveraging large language models to build cutting-edge AI applications. In this blog post, we ...', \"LangChain connects LLMs to your company's private data and APIs to build context-aware, reasoning applications. Rapidly move from prototype to production with ...\", 'LangChain is an amazing framework to get LLM projects done in a matter of no time, and the ecosystem is growing fast. Here is an attempt to keep track of ...', 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual ...', 'LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m It seems like Langchain has a wide range of use-cases, including document analysis, chatbots, and code analysis. It also has a flexible framework for integrating language models with company data and APIs.\n",
      "Thought: I now know the final answer.\n",
      "Final Answer: Langchain is a software framework for building applications with large language models, with use-cases including document analysis, chatbots, and code analysis. It also has a flexible framework for integrating language models with company data and APIs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Langchain is a software framework for building applications with large language models, with use-cases including document analysis, chatbots, and code analysis. It also has a flexible framework for integrating language models with company data and APIs.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for the company name and \"CEO\"\n",
      "Action: Search\n",
      "Action Input: \"pipe CEO\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLuke Voiles\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should verify this information\n",
      "Action: Search\n",
      "Action Input: \"Luke Voiles\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"Experience: SavvyMoney · Location: San Francisco · 500+ connections on LinkedIn. View Luke Voiles' profile on LinkedIn, a professional community of 1 ...\", 'Luke has been around fintech for many years and he became CEO of Pipe about a year ago now. He has a different perspective on small business lending.', \"Fintech Explained Q&A with Luke Voiles, CEO at Pipe · As Pipe's Chief Executive Officer, can you share a bit about Pipe and the company's mission ...\", 'I feel lucky to get to be here to try help this team succeed in our purpose to empower financial freedom for business owners.', \"Pipe, creator of the world's first trading platform for revenues, today announced Luke Voiles will become the company's next Chief Executive Officer.\", 'Luke Voiles is vice president and business leader of Quickbooks Capital at Intuit.', 'In this insightful interview Luke Voiles is shaking up the business financing space with exciting embedded capital platform Pipe.', \"Pipe's CEO Luke Voiles joins the podcast to discuss the firm's move into embedded lending with its new Capital as a Service platform.\"]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This information seems reliable and consistent\n",
      "Final Answer: The CEO of Pipe is Luke Voiles.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The CEO of Pipe is Luke Voiles.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input should be a search query.\n",
    "agent.run(\"Who is the CEO of pipe?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WOLFRAM_ALPHA_APPID\"] = \"8P6KPJ-JUK8PVV723\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\"wolfram-alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input should be a search query.\n",
    "\n",
    "agent.run(\"What is the asthenosphere?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
